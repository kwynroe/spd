C: 32
batch_size: 2048
embedding_recon_coeff: null
faithfulness_coeff: 10.0
image_freq: 5000
image_on_first_step: true
importance_minimality_coeff: 0.001
is_embed_unembed_recon: false
log_ce_losses: false
lr: 0.003
lr_exponential_halflife: null
lr_schedule: cosine
lr_warmup_pct: 0.0
n_ci_mlp_neurons: 32
n_eval_steps: 100
n_mask_samples: 1
out_recon_coeff: null
output_loss_type: attn
pnorm: 1.0
pretrained_model_class: spd.experiments.attention.models.SingleHeadAttentionModel
pretrained_model_name_hf: null
pretrained_model_output_attr: null
pretrained_model_path: spd/experiments/attention/toy_out/attention_vocab20_d16_seq8_datarandom_seed0_20250707_170009_662/attention_model.pth
print_freq: 5000
recon_coeff: null
recon_layerwise_coeff: null
save_freq: null
schatten_coeff: null
seed: 0
steps: 50000
stochastic_recon_coeff: 5.0
stochastic_recon_layerwise_coeff: 5.0
target_module_patterns:
- attention
task_config:
  n_trigrams: 32
  seq_len: 8
  task_name: attention
  vocab_size: 20
tokenizer_name: null
wandb_entity: kwyn390
wandb_project: spd-attn
wandb_run_name: null
wandb_run_name_prefix: ''
