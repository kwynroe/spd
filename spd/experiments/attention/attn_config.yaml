# TMS 5-2
# --- WandB ---
wandb_entity: kwyn390
wandb_project: spd-attn
wandb_run_name: null
wandb_run_name_prefix: ""

# --- General ---
seed: 0
C: 32
n_mask_samples: 1
n_ci_mlp_neurons: 32
# n_ci_mlp_neurons: 0
# target_module_patterns: ["linear1", "linear2"]
target_module_patterns: ["attention"]

# --- Loss Coefficients ---
faithfulness_coeff: 10
recon_coeff: null
stochastic_recon_coeff: 5.0
recon_layerwise_coeff: null
stochastic_recon_layerwise_coeff: 5.0
importance_minimality_coeff: 1e-3
pnorm: 1.0
output_loss_type: "attn"

# --- Training ---
batch_size: 2048
steps: 50_000
lr: 3e-3
lr_schedule: cosine
lr_warmup_pct: 0.0
n_eval_steps: 100

# --- Logging & Saving ---
image_freq: 5_000
print_freq: 5000
save_freq: null

# --- Pretrained model info ---
pretrained_model_class: "spd.experiments.attention.models.SingleHeadAttentionModel"
# pretrained_model_path: "wandb:spd-train-tms/runs/bohlithe"
pretrained_model_path: "/root/spd/spd/experiments/attention/toy_out/attention_vocab20_d16_seq8_datarandom_seed0_20250707_170009_662/attention_model.pth" # 1 hidden w/fixed identity

task_config:
  task_name: "attention"
  vocab_size: 20
  seq_len: 8
  n_trigrams: 32
